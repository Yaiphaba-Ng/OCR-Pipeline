{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detector (FAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1346/896590155.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  det_params = torch.load(detector_model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor, fast_base, detection_predictor\n",
    "\n",
    "os.environ['USE_TORCH'] = '1'\n",
    "print(os.environ['USE_TORCH'])\n",
    "\n",
    "# Initialize the default OCR predictor\n",
    "# default_predictor = detection_predictor('fast_base',pretrained=True)\n",
    "\n",
    "# Load custom detection model\n",
    "detector_model_path = \"checkpoints/FAST/fast_base_20240926-134200_epoch100.pt\"\n",
    "det_model = fast_base(pretrained=False, pretrained_backbone=False)\n",
    "det_params = torch.load(detector_model_path, map_location=\"cpu\")\n",
    "det_model.load_state_dict(det_params)\n",
    "trained_predictor = ocr_predictor(det_arch=det_model, reco_arch=\"crnn_vgg16_bn\", pretrained=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise Random Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def process_random_images_from_directory(directory, trained_predictor, num_images=5):\n",
    "    # List all image files in the directory\n",
    "    image_filenames = [f for f in os.listdir(directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    # Select 5 random images\n",
    "    random_filenames = random.sample(image_filenames, num_images)\n",
    "    \n",
    "    # Process each randomly selected image\n",
    "    for image_filename in random_filenames:\n",
    "        image_path = os.path.join(directory, image_filename)\n",
    "        print(f\"Processing: {image_filename}\")\n",
    "        \n",
    "        # Load and process the image using doctr's DocumentFile and trained_predictor\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        result = trained_predictor(doc)\n",
    "        \n",
    "        # Display the result (or handle as needed)\n",
    "        result.show()\n",
    "\n",
    "# Example usage:\n",
    "# directory = \"old datasets\\Grayscaled nielit form\\Processed\"\n",
    "directory = \"files/input/Single Sample Inference\"\n",
    "process_random_images_from_directory(directory, trained_predictor, num_images=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract BBOX in COCO Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from doctr.io import DocumentFile\n",
    "from PIL import Image\n",
    "\n",
    "image_dir = \"files/input/Single Sample Inference\"\n",
    "output_file = \"files/input/Single Sample Inference/detection.json\"\n",
    "# model = default_predictor\n",
    "model = trained_predictor\n",
    "\n",
    "# Initialize COCO data structure\n",
    "coco_output = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"word\",\n",
    "        \"supercategory\": \"text\",\n",
    "    }]\n",
    "}\n",
    "\n",
    "annotation_id = 1  # Unique annotation ID\n",
    "image_id = 1  # Unique image ID\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        # Load the document and image\n",
    "        doc = DocumentFile.from_images(image_path)\n",
    "        image = Image.open(image_path)\n",
    "        width, height = image.size\n",
    "\n",
    "        # Add image info to COCO structure\n",
    "        coco_output['images'].append({\n",
    "            \"file_name\": filename,\n",
    "            \"height\": height,\n",
    "            \"width\": width,\n",
    "            \"id\": image_id\n",
    "        })\n",
    "\n",
    "        # Get the result from the model\n",
    "        result = model(doc)\n",
    "        # Access the blocks of text in the document prediction\n",
    "        for page in result.pages:\n",
    "            for block in page.blocks:\n",
    "                for line in block.lines:\n",
    "                    for word in line.words:\n",
    "                        # Get the bounding box\n",
    "                        bbox = word.geometry  # [x_min, y_min, x_max, y_max]\n",
    "                        bbox_coco = [\n",
    "                            bbox[0][0] * width,  # x_min\n",
    "                            bbox[0][1] * height,  # y_min\n",
    "                            (bbox[1][0] - bbox[0][0]) * width,  # width\n",
    "                            (bbox[1][1] - bbox[0][1]) * height  # height\n",
    "                        ]\n",
    "\n",
    "                        # Append annotation to COCO structure\n",
    "                        coco_output['annotations'].append({\n",
    "                            \"id\": annotation_id,\n",
    "                            \"image_id\": image_id,\n",
    "                            \"category_id\": 1,  # Word category\n",
    "                            \"bbox\": bbox_coco,\n",
    "                            \"area\": bbox_coco[2] * bbox_coco[3],\n",
    "                            \"iscrowd\": 0\n",
    "                        })\n",
    "\n",
    "                        annotation_id += 1\n",
    "\n",
    "        image_id += 1\n",
    "\n",
    "# Write the COCO JSON structure to the output file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(coco_output, file)\n",
    "\n",
    "print(f\"COCO annotations saved in {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1346/4133574793.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  det_params = torch.load(detector_model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COCO annotations saved in files/input/Single Sample Inference/detection.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from doctr.io import DocumentFile\n",
    "from doctr.models import ocr_predictor, fast_base, detection_predictor\n",
    "\n",
    "os.environ[\"USE_TORCH\"] = \"1\"\n",
    "print(os.environ[\"USE_TORCH\"])\n",
    "\n",
    "# Initialize the default OCR predictor\n",
    "# default_predictor = detection_predictor('fast_base',pretrained=True)\n",
    "\n",
    "# Load custom detection model\n",
    "detector_model_path = \"checkpoints/FAST/fast_base_20240926-134200_epoch100.pt\"\n",
    "det_model = fast_base(pretrained=False, pretrained_backbone=False)\n",
    "det_params = torch.load(detector_model_path, map_location=\"cpu\")\n",
    "det_model.load_state_dict(det_params)\n",
    "\n",
    "trained_predictor = ocr_predictor(\n",
    "    det_arch=det_model, \n",
    "    reco_arch=\"crnn_vgg16_bn\", \n",
    "    pretrained=True\n",
    ")\n",
    "# image_dir = \"files/input/Single Sample Inference\"\n",
    "image_path = \"files/input/Single Sample Inference/0001_front.jpg\"\n",
    "output_file = \"files/input/Single Sample Inference/detection.json\"\n",
    "# model = default_predictor\n",
    "model = trained_predictor\n",
    "\n",
    "# Initialize COCO data structure\n",
    "coco_output = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": [{\n",
    "        \"id\": 1,\n",
    "        \"name\": \"word\",\n",
    "        \"supercategory\": \"text\",\n",
    "    }]\n",
    "}\n",
    "\n",
    "annotation_id = 1  # Unique annotation ID\n",
    "image_id = 1  # Unique image ID\n",
    "\n",
    "# Load the document and image\n",
    "doc = DocumentFile.from_images(image_path)\n",
    "image = Image.open(image_path)\n",
    "width, height = image.size\n",
    "\n",
    "# Add image info to COCO structure\n",
    "coco_output[\"images\"].append(\n",
    "    {\n",
    "        \"file_name\": os.path.basename(image_path),\n",
    "        \"height\": height,\n",
    "        \"width\": width,\n",
    "        \"id\": image_id,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get the result from the model\n",
    "result = model(doc)\n",
    "# Access the blocks of text in the document prediction\n",
    "for page in result.pages:\n",
    "    for block in page.blocks:\n",
    "        for line in block.lines:\n",
    "            for word in line.words:\n",
    "                # Get the bounding box\n",
    "                bbox = word.geometry  # [x_min, y_min, x_max, y_max]\n",
    "                bbox_coco = [\n",
    "                    bbox[0][0] * width,  # x_min\n",
    "                    bbox[0][1] * height,  # y_min\n",
    "                    (bbox[1][0] - bbox[0][0]) * width,  # width\n",
    "                    (bbox[1][1] - bbox[0][1]) * height,  # height\n",
    "                ]\n",
    "\n",
    "                # Append annotation to COCO structure\n",
    "                coco_output[\"annotations\"].append(\n",
    "                    {\n",
    "                        \"id\": annotation_id,\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": 1,  # Word category\n",
    "                        \"bbox\": bbox_coco,\n",
    "                        \"area\": bbox_coco[2] * bbox_coco[3],\n",
    "                        \"iscrowd\": 0,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                annotation_id += 1\n",
    "\n",
    "# Write the COCO JSON structure to the output file\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(coco_output, file)\n",
    "\n",
    "print(f\"COCO annotations saved in {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognizer (TrOCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    x: float\n",
    "    y: float\n",
    "    width: float\n",
    "    height: float\n",
    "\n",
    "    @property\n",
    "    def left(self) -> float:\n",
    "        return self.x\n",
    "\n",
    "    @property\n",
    "    def right(self) -> float:\n",
    "        return self.x + self.width\n",
    "\n",
    "    @property\n",
    "    def top(self) -> float:\n",
    "        return self.y\n",
    "\n",
    "    @property\n",
    "    def bottom(self) -> float:\n",
    "        return self.y + self.height\n",
    "\n",
    "def load_coco_annotations(file_path: str) -> Dict:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def merge_line_boxes(\n",
    "    boxes: List[BoundingBox], distance_threshold: float = 20\n",
    ") -> List[BoundingBox]:\n",
    "    if not boxes:\n",
    "        return []\n",
    "\n",
    "    # Sort boxes by their top-left corner\n",
    "    sorted_boxes = sorted(boxes, key=lambda b: (b.top, b.left))\n",
    "    merged_boxes = []\n",
    "    current_line = [sorted_boxes[0]]\n",
    "\n",
    "    for box in sorted_boxes[1:]:\n",
    "        last_box = current_line[-1]\n",
    "\n",
    "        # Check if the box is on the same line\n",
    "        if (\n",
    "            abs(box.top - last_box.top) <= distance_threshold\n",
    "            and abs(box.bottom - last_box.bottom) <= distance_threshold\n",
    "            and (box.left - last_box.right) <= distance_threshold\n",
    "        ):\n",
    "            current_line.append(box)\n",
    "        else:\n",
    "            # Merge the current line and start a new one\n",
    "            merged_boxes.append(merge_boxes(current_line))\n",
    "            current_line = [box]\n",
    "\n",
    "    # Merge the last line\n",
    "    if current_line:\n",
    "        merged_boxes.append(merge_boxes(current_line))\n",
    "\n",
    "    return merged_boxes\n",
    "\n",
    "def merge_boxes(boxes: List[BoundingBox]) -> BoundingBox:\n",
    "    left = min(box.left for box in boxes)\n",
    "    top = min(box.top for box in boxes)\n",
    "    right = max(box.right for box in boxes)\n",
    "    bottom = max(box.bottom for box in boxes)\n",
    "    return BoundingBox(left, top, right - left, bottom - top)\n",
    "\n",
    "def process_coco_annotations(annotations: List[Dict]) -> List[BoundingBox]:\n",
    "    boxes = [BoundingBox(*ann[\"bbox\"]) for ann in annotations]\n",
    "    return merge_line_boxes(boxes)\n",
    "\n",
    "def create_merged_coco_annotations(\n",
    "    original_coco: Dict, merged_boxes: List[BoundingBox]\n",
    ") -> Dict:\n",
    "    merged_coco = original_coco.copy()\n",
    "    merged_coco[\"annotations\"] = [\n",
    "        {\n",
    "            \"id\": i + 1,\n",
    "            \"image_id\": original_coco[\"annotations\"][0][\"image_id\"],\n",
    "            \"category_id\": 1,\n",
    "            \"bbox\": [box.x, box.y, box.width, box.height],\n",
    "            \"area\": box.width * box.height,\n",
    "            \"iscrowd\": 0,\n",
    "        }\n",
    "        for i, box in enumerate(merged_boxes)\n",
    "    ]\n",
    "    merged_coco[\"categories\"] = [\n",
    "        {\"id\": 1, \"name\": \"text_line\", \"supercategory\": \"text\"}\n",
    "    ]\n",
    "    return merged_coco\n",
    "\n",
    "def save_coco_annotations(coco_data: Dict, output_file_path: str):\n",
    "    with open(output_file_path, \"w\") as f:\n",
    "        json.dump(coco_data, f, indent=2)\n",
    "\n",
    "def main(input_coco_file_path: str, output_coco_file_path: str):\n",
    "    original_coco = load_coco_annotations(input_coco_file_path)\n",
    "    merged_boxes = process_coco_annotations(original_coco[\"annotations\"])\n",
    "\n",
    "    print(f\"Number of original boxes: {len(original_coco['annotations'])}\")\n",
    "    print(f\"Number of merged line boxes: {len(merged_boxes)}\")\n",
    "\n",
    "    merged_coco = create_merged_coco_annotations(original_coco, merged_boxes)\n",
    "    save_coco_annotations(merged_coco, output_coco_file_path)\n",
    "\n",
    "    print(f\"Merged COCO annotations saved to: {output_coco_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_coco_file_path = \"./files/input/Single Sample Inference/detection.json\"\n",
    "    output_coco_file_path = \"./files/input/Single Sample Inference/merged_coco_result.json\"\n",
    "    main(input_coco_file_path, output_coco_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to convert COCO bbox to a polygon (rectangle)\n",
    "def bbox_to_polygon(bbox):\n",
    "    x, y, width, height = bbox\n",
    "    return [\n",
    "        [x, y], \n",
    "        [x + width, y], \n",
    "        [x + width, y + height], \n",
    "        [x, y + height]\n",
    "    ]\n",
    "\n",
    "# Core function to visualize the annotations for a single image\n",
    "def visualize_single_image(json_file, image_folder, image_filename):\n",
    "    # Load the labels data from the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    \n",
    "    # Find the corresponding image data\n",
    "    image_data = next((img for img in labels['images'] if img['file_name'] == image_filename), None)\n",
    "    \n",
    "    if not image_data:\n",
    "        print(f\"Image {image_filename} not found in JSON file.\")\n",
    "        return\n",
    "    \n",
    "    image_id = image_data['id']\n",
    "    \n",
    "    # Load the image\n",
    "    image_path = os.path.join(image_folder, image_filename)\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Failed to load image {image_filename}.\")\n",
    "        return\n",
    "    \n",
    "    # Retrieve annotations (bounding boxes) for the given image\n",
    "    annotations = [ann for ann in labels['annotations'] if ann['image_id'] == image_id]\n",
    "    \n",
    "    # Draw the bounding boxes on the image\n",
    "    for ann in annotations:\n",
    "        bbox = ann['bbox']\n",
    "        polygon = bbox_to_polygon(bbox)\n",
    "        polygon_np = np.array(polygon, np.int32).reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [polygon_np], isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "    \n",
    "    # Convert the image from BGR (OpenCV format) to RGB (for Matplotlib)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display the annotated image\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis('off')  # Hide axis\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "json_file = \"./files/input/Single Sample Inference/merged_coco_result.json\"\n",
    "image_folder = 'files/input/Single Sample Inference'\n",
    "image_filename = '0001_front.jpg'\n",
    "visualize_single_image(json_file, image_folder, image_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as v2\n",
    "from transformers import VisionEncoderDecoderModel, TrOCRProcessor\n",
    "# import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_annotations(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 8)\n",
    "    denoised = cv2.fastNlMeansDenoising(thresh, None, 10, 7, 21)\n",
    "    rgb = cv2.cvtColor(denoised, cv2.COLOR_GRAY2RGB)\n",
    "    return Image.fromarray(rgb)\n",
    "\n",
    "def load_model_and_processor(model_path):\n",
    "    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return processor, model, device\n",
    "\n",
    "def process_image(img, bbox, processor, model, device, transform):\n",
    "    # Crop image based on bounding box\n",
    "    x, y, w, h = map(int, bbox)\n",
    "    cropped_img = img.crop((x, y, x+w, y+h))\n",
    "\n",
    "    # Preprocess\n",
    "    preprocessed_img = preprocess_image(cropped_img)\n",
    "\n",
    "    # Transform and move to device\n",
    "    img_t = transform(preprocessed_img).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(img_t)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "def main(coco_file_path, image_folder, model_path):\n",
    "    # Load COCO annotations\n",
    "    coco_data = load_coco_annotations(coco_file_path)\n",
    "\n",
    "    # Load model and processor\n",
    "    processor, model, device = load_model_and_processor(model_path)\n",
    "\n",
    "    # Define transform\n",
    "    transform = v2.Compose([\n",
    "        v2.Resize((384,384)),\n",
    "        v2.ToTensor(),\n",
    "        v2.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    # Process each image\n",
    "    for image_info in coco_data['images']:\n",
    "        image_path = os.path.join(image_folder, image_info['file_name'])\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Get annotations for this image\n",
    "        image_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_info['id']]\n",
    "\n",
    "        # Sort annotations from top-left to bottom-right\n",
    "        image_annotations.sort(key=lambda x: (x['bbox'][1], x['bbox'][0]))\n",
    "\n",
    "        print(f\"Processing image: {image_info['file_name']}\")\n",
    "\n",
    "        # # Display the full image\n",
    "        # plt.figure(figsize=(12, 12))\n",
    "        # plt.imshow(img)\n",
    "        # plt.axis('off')\n",
    "        # plt.title(f\"Full Image: {image_info['file_name']}\")\n",
    "        # plt.show()\n",
    "\n",
    "        # Process each bounding box and print recognized text\n",
    "        for ann in image_annotations:\n",
    "            generated_text = process_image(img, ann['bbox'], processor, model, device, transform)\n",
    "            print(f\"Recognized Text: {generated_text}\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    coco_file_path = output_coco_file_path\n",
    "    image_folder = \"./files/input/Single Sample Inference\"\n",
    "    model_path = \"./checkpoints/TrOCR\"\n",
    "    main(coco_file_path, image_folder, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCR-Pipeline-Poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
