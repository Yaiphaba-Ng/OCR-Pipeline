{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doctr.models import ocr_predictor\n",
    "from PIL import Image\n",
    "import torch\n",
    "from doctr.models import ocr_predictor, fast_base, detection_predictor\n",
    "import numpy as np\n",
    "# from transformers import LayoutLMv3Processor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "det_model = fast_base(pretrained=False, pretrained_backbone=False)\n",
    "det_params = torch.load('F:/LayoutLMV3/pretrained_model/fast_base_20240926-134200_epoch100.pt', map_location=\"cpu\")\n",
    "det_model.load_state_dict(det_params)\n",
    "ocr_model = ocr_predictor(det_arch=det_model, reco_arch=\"crnn_vgg16_bn\", pretrained=True)\n",
    "\n",
    "# Load your image\n",
    "img = Image.open('layoutlmv3/0001_front.jpg')\n",
    "width, height = img.size\n",
    "print(\"Original Image Size:\", width, height)  # Debug print\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Perform OCR on the image\n",
    "result = ocr_model([img_np])\n",
    "\n",
    "# Check the overall structure of the OCR result\n",
    "# print(\"OCR Result Structure:\", result)\n",
    "\n",
    "# Extract tokens and bounding boxes\n",
    "tokens = []\n",
    "boxes = []\n",
    "\n",
    "for block in result.pages[0].blocks:\n",
    "    for line in block.lines:\n",
    "        for word in line.words:\n",
    "            tokens.append(word.value)\n",
    "            geometry = word.geometry\n",
    "            \n",
    "            if geometry:\n",
    "                # Calculate bounding box coordinates\n",
    "                x_min = min(p[0] for p in geometry)\n",
    "                y_min = min(p[1] for p in geometry)\n",
    "                x_max = max(p[0] for p in geometry)\n",
    "                y_max = max(p[1] for p in geometry)\n",
    "\n",
    "                # Check for valid bounding box\n",
    "                if x_min < x_max and y_min < y_max:\n",
    "                    # Scale to 0-1000 range\n",
    "                    boxes.append([x_min * 1000, y_min * 1000, x_max * 1000, y_max * 1000])\n",
    "                else:\n",
    "                    print(f\"Invalid bounding box for word '{word.value}': [{x_min}, {y_min}, {x_max}, {y_max}]\")\n",
    "            else:\n",
    "                print(f\"No geometry found for word '{word.value}'\")\n",
    "                \n",
    "# Filter invalid bounding boxes\n",
    "valid_tokens = []\n",
    "valid_boxes = []\n",
    "\n",
    "for token, box in zip(tokens, boxes):\n",
    "    if box != [0, 0, 0, 0]:\n",
    "        valid_tokens.append(token)\n",
    "        valid_boxes.append(box)\n",
    "\n",
    "boxes_tensor = torch.tensor(valid_boxes, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoProcessor\n",
    "\n",
    "# Load the model and processor for NER\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"test/checkpoint-500\")\n",
    "processor = AutoProcessor.from_pretrained(\"test/checkpoint-500\")\n",
    "\n",
    "def unnormalize_box(bbox, width, height):\n",
    "     return [\n",
    "         width * (bbox[0] / 1000),\n",
    "         height * (bbox[1] / 1000),\n",
    "         width * (bbox[2] / 1000),\n",
    "         height * (bbox[3] / 1000),\n",
    "     ]\n",
    "    \n",
    "if len(valid_tokens) != len(boxes_tensor):\n",
    "    print(f\"Warning: Number of tokens ({len(valid_tokens)}) does not match number of boxes ({len(boxes_tensor)})!\")\n",
    "else:\n",
    "    # Create encoding for the model without padding\n",
    "    encoding = processor(\n",
    "        img, \n",
    "        valid_tokens, \n",
    "        boxes=boxes_tensor, \n",
    "        return_tensors=\"pt\", \n",
    "        padding=False  # Disable automatic padding\n",
    "    )\n",
    "\n",
    "    # Check the shape of input tensors after encoding\n",
    "#     print(f\"input_ids shape: {encoding['input_ids'].shape}\")\n",
    "#     print(f\"attention_mask shape: {encoding['attention_mask'].shape}\")\n",
    "#     print(f\"bbox shape: {encoding['bbox'].shape}\")\n",
    "#     print(f\"pixel_values shape: {encoding['pixel_values'].shape}\")\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoding)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(-1).squeeze().tolist()\n",
    "\n",
    "    # Print some predictions for debugging\n",
    "#     print(f\"Predictions: {predictions[:5]}\")\n",
    "encoded_valid_tokens = encoding['input_ids'].squeeze().tolist()\n",
    "encoded_valid_boxes = encoding['bbox'].squeeze().tolist()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OCR-Pipeline-Poetry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
